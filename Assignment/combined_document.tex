\documentclass{article}
\usepackage{amsmath}
\begin{document}



\section*{6. Skolem's Method}
In this section we study the application of Skolem's method to equations of the form
\begin{equation}
F(x_1, \ldots, x_m) = c,
\end{equation}
where $F$ is an irreducible, decomposable, nonfull form (Section 1.3 of Chapter 2), and $c$ is a rational number. This method is based on some simple properties of local analytic manifolds over $p$-adic fields, which will be proved in the next section. An example which illustrates the idea of Skolem's method was given at the beginning of this chapter.

\subsection*{6.1. Representation of Numbers by Nonfull Decomposable Forms}
In Section 1.3 of Chapter 2 we saw that (6.1) can be written in the form
\begin{equation}
N(x_1\mu_1 + \cdots + x_m\mu_m) = a
\end{equation}
or
\begin{equation}
N(\alpha) = a, \quad (\alpha \in M),
\end{equation}
where $\mu_1, \ldots, \mu_m$ are numbers of some algebraic number field $k$, and $M = \{\mu_1, \ldots, \mu_m\}$ is the module generated by these numbers ($a$ is a rational number). Replacing, if necessary, the form $F$ by a form integrally equivalent to it, we may assume that the numbers $\mu_1, \ldots, \mu_m$ of the module $M$ are linearly independent over the field $R$ of rational numbers. Since $M$ is nonfull, $m < n = (k : R)$.

In Chapter 2 we saw how to find all solutions to (6.3) when $M$ is a full module of $k$. It is thus natural to embed $M$ in a full module $M$, and to use the methods of Chapter 2 to find all solutions of the equation $N(\alpha) = a, \alpha \in M$, and then to pick out those solutions which lie in $M$.

It is clear that any module of $k$ can be embedded in a full module. To do this it suffices to extend the linearly independent set $\mu_1, \ldots, \mu_m$ to a basis $\mu_1, \ldots, \mu_n$ of the field $k$ and to set $M = \{\mu_1, \ldots, \mu_n\}$.

If all $\alpha \in M$ for which $N(\alpha) = a$ have already been found, then we shall obtain all solutions of (6.3) if we can isolate those solutions for which in the representation
\begin{equation}
\alpha = x_1\mu_1 + \cdots + x_n\mu_n
\end{equation}
the coefficients $x_{m+1}, \ldots, x_n$ are equal to zero. To express the conditions $x_{m+1} = \cdots = x_n = 0$ directly in terms of $\alpha$, it is convenient to use the dual basis $\mu_1^*, \ldots, \mu_n^*$ (see Section 2.3 of the Supplement). Since the trace $\text{Sp }\mu_i^*\mu_j^* \neq 0$ for $i \neq j$ and 1 for $i = j$, then $x_i = \text{Sp }\alpha \mu_i^* (1 \leq i \leq n)$. It follows that the 



the coefficients $x_i, \ldots, x_n$ are equal to zero. To express the conditions 
$x_{m+1} = \cdots = x_n$ directly in terms of $\alpha$, it is convenient to use 
the dual basis $\mu_1^*, \ldots, \mu_n^*$ (see Section 2.3 of the Supplement). 
Since the trace $\text{Sp} \mu_i^* \mu_j^* \neq 0$ for $i \neq j$ and $1$ for $i = j$, 
then $x_i = \text{Sp} \alpha \mu_i^* (1 \leq i \leq n)$. It follows that the 

The set of all numbers of the field $k$ of the form $\epsilon_1^{u_1} \cdots \epsilon_r^{u_r}$, 
where $u_1, \ldots, u_r$ run through all rational integers, is a multiplicative subgroup of $k$ 
which we shall denote by $U$. The solutions of (6.3) then coincide with numbers of the intersections 
\begin{equation}
M \cap \gamma_j U \quad (j = 1, \ldots, k).
\end{equation}
Instead of the set (6.8) we may also consider the similar set $\gamma_j^{-1} M \cap U$. 
Then the problem of finding all solutions to (6.1) is reduced to the problem of finding 
the intersection of a module and a multiplicative subgroup of the field $k$. We note also 
that we may replace the module $M$ by the vector space $L$ (over $R$) which is spanned by 
$\mu_1, \ldots, \mu_m$. For $\gamma_j U \subseteq M$ and $L \cap M = M$, so 
\begin{equation}
L \cap \gamma_j U = M \cap \gamma_j U.
\end{equation}

\subsection*{6.2. The Relation to Local Analytic Manifolds}
The idea of Skolem's method is that in some cases we can prove the finiteness of the number 
of solutions of (6.1) by proving that the system (6.7)




has only a finite number of solutions even when the variables $u_1, \ldots, u_r$ take 
on $\mathbb{Q}$-adic integral values (that is, they take integral values in the completion 
$K_{\mathfrak{p}}$, where $\mathbb{Q}$ is any prime divisor of the field $K$). After such an 
extension of the domain of possible values for the variables, we may consider the set of all 
solutions of the system (6.7) as a local analytic manifold in $r$-dimensional space, and then 
apply properties of such manifolds.

When we allow that variables $u_1, \ldots, u_r$ in (6.7) to take $\mathbb{Q}$-adic values, 
we encounter the obstacle that the exponential function $\epsilon^u = \exp(u \log \epsilon)$ 
is only defined for all integral $\mathbb{Q}$-adic $u$ when $\epsilon \equiv 1 \pmod{\mathbb{Q}^k}$ 
($\kappa$ is an integer which depends only on the field $K_{\mathfrak{p}}$; see the end of Section 5). 
We avoid this difficulty in the following manner. By Problem 6 of Section 7, Chapter 3, 
there exists a natural number $q$ such that any integer $\alpha \in K$ which is not divisible 
by $\mathbb{Q}$ satisfies
\begin{equation}
\alpha^q \equiv 1 \pmod{\mathbb{Q}^k}.
\end{equation}

Each exponent $u_i$ in (6.5) can be written
\begin{equation}
u_i = \rho_i + q v_i, \quad (0 \leq \rho_i < q, v_i \in \mathbb{Z}),
\end{equation}
and hence the unit $\epsilon = \epsilon_1^{u_1} \cdots \epsilon_r^{u_r}$ has the representation
\begin{equation}
\epsilon = \delta \epsilon_1^{\rho_1} \cdots \epsilon_r^{\rho_r} \cdot \epsilon_r^{q v_r},
\end{equation}
where $\delta$ is one of the $q^r$ numbers
\begin{equation}
\epsilon_1^{\rho_1} \cdots \epsilon_r^{\rho_r}, \quad (0 \leq \rho_i < q).
\end{equation}

Hence we obtain a new representation for numbers $\alpha$ of the form (6.5), 
in which $\epsilon_i$ is replaced by $\epsilon_i^q$, and the finite set of numbers $\gamma_j$ 
by the set of numbers $\gamma_j \delta$. Since the $\epsilon_i$ are units, then the congruence 
(6.9) holds for all of the numbers $\sigma_j(\epsilon_i)$, and hence the functions $\sigma_j(\epsilon_i)^u$ 
are defined for all $\mathbb{Q}$-adic integers $u \in K_{\mathfrak{p}}$. We have proved the following result.

\subsection*{Lemma 1.}
After making new choices, if necessary, for the numbers $\gamma_j$ and $\epsilon_i$ in (6.5), 
the functions $\sigma_j(\epsilon_i)^u$ are defined for all integers of the field $K_{\mathfrak{p}}$.

In the future we shall assume that this condition holds without special mention.

We turn to the system (6.7). In view of (5.9) and (5.13), we can put this system in the form
\begin{equation}
\sum_{j=1}^r A_{ij} \exp L_j(v_1, \ldots, v_r) = 0 \quad (i = m + 1, \ldots, n),
\end{equation}




where 
\begin{equation}
L_j(u_1, \ldots, u_r) = \sum_{k=1}^r u_k \log \sigma_j(\epsilon_k),
\end{equation}
\begin{equation}
A_{ij} = \sigma_j(\mu_i^*).
\end{equation}

Since the left side of (6.10) consists of power series which converge for all $\mathbb{Q}$-adic 
integral $u_1, \ldots, u_r$, and hence represent analytic functions, then the set of all solutions 
to (6.10) can be interpreted as a local analytic manifold (in a neighborhood of any solution) in 
the sense of the definition of Section 7.

The system (6.10) consists of $n - m$ equations in $r$ variables. It is natural to expect that the 
manifold defined by this system will consist of only a finite number of points, provided that $n - m > r$. 
Recall that the number $r$ comes from the Dirichlet theorem on units, and $r = s + t - 1$, where $s$ is the 
number of real, and $t$ the number of pairs, of complex embeddings of the field $k$ in the field of complex 
numbers. Since $n = s + 2t$, then $n - m > r$ if and only if $t > 1$. In the simplest interesting case $m = 2$, 
and the condition reduces to $t > 1$. This means that there should be at least one pair of complex embeddings 
of $k$. This case leads to Thue's theorem, and will be considered in the next sections.

Assume that the system (6.10) has an infinite number of solutions $(u_1^*, \ldots, u_r^*)$, $s = 1, 2, \ldots$. 
Since the ring of $\mathbb{Q}$-adic integers is compact (see Theorem 6 of Section 3, Chapter 1, and the second 
remark at the end of Section 1.2 of this chapter), we can choose from this sequence a convergent subsequence, 
the limit of which we denote by $(u_1^*, \ldots, u_r^*)$. It is clear that the point $(u_1^*, \ldots, u_r^*)$ is 
also a solution of (6.10), that is, that it lies on the manifold determined by this system, and that in any 
neighborhood of this point there are an infinite number of points of the manifold. We now change variables by 
the formula
\begin{equation}
u_i = u_i^* + v_i \quad (1 \leq i \leq r).
\end{equation}
The system (7.10) then becomes
\begin{equation}
\sum_{j=1}^r A_{ij} \exp L_j(v_1, \ldots, v_r) = 0 \quad (i = m + 1, \ldots, n),
\end{equation}
where
\begin{equation}
A_{ij}^* = A_{ij} \exp L_j(u_1^*, \ldots, u_r^*).
\end{equation}
The constant terms of the series on the left of (6.11) are all zero. We denote by $V$ the local analytic 
manifold (see Section 7) determined by (6.11) [in the neighborhood of the point $(0, \ldots, 0)$]. Since this 
manifold does not consist of a single point (any neighborhood of the origin contains an infinite number of points 
of the manifold), then by Theorem 2 of Section 7 the manifold $V$



has only a finite number of solutions to (6.1) to the proof that the systems 
(6.11) do not have solutions in formal power series, that is, that the corresponding 
local analytic manifolds do not contain analytic curves.

Note that there are $n - r$ linear relations on the $n$ series $P_j(t)$ defined by 
\begin{equation}
\sum_{j=1}^n B_{ij} P_j(t) = 0 \quad (1 \leq i \leq n - r),
\end{equation}
since they are linear combinations of the $r$ series $\omega_i(t)$. Thus the existence of an 
analytic curve on $V$ implies the solvability [in power series $P_j(t)$ without constant term] 
of the system
\begin{equation}
\sum_{j=1}^n A_{ij}^* \exp P_j(t) = 0 \quad (m + 1 \leq i \leq n),
\end{equation}
\begin{equation}
\sum_{j=1}^n B_{ij} P_j(t) = 0 \quad (1 \leq i \leq n - r = t + 1),
\end{equation}
in which both groups of equations are linearly independent. [The linear independence of the 
equations of the first group follows from the fact that the determinant 
\begin{equation}
\sigma(\gamma \mu_1^*, \ldots, \gamma \mu_r^*),
\end{equation}
whose square is the discriminant of the basis $\gamma \mu_1^*$, is nonzero, and hence the rank 
of the matrix $(A_{ij}^*) (m + 1 \leq i \leq n, 1 \leq j \leq n)$, and, consequently, of the 
matrix $(A_{ij}^*)$, is $n - m.]$ If we assume that $n - m \geq r$, then the total number of 
equations in (6.13) will be $\geq n$.

This theorem is the heart of Skolem's method. It reduces the question of the finiteness of the 
number of solutions to (6.1) to the proof that the systems (6.11) do not have solutions in formal 
power series, that is, that the corresponding local analytic manifolds do not contain analytic curves.

The rest of the proof of Theorem 2 is based on the following important auxiliary result.




\subsection*{6.3. Thue's Theorem}
Thue's theorem states that if the form $f(x, y) = a_0 x^n + a_1 x^{n-1} y + \cdots + a_n y^n$ 
in two variables with rational integral coefficients is irreducible and has degree $n \geq 3$, 
then the equation
\begin{equation}
f(x, y) = c
\end{equation}
has only a finite number of solutions in integers. Since forms in two variables are always decomposable, 
and when $n \geq 2$ they are nonfull, then the equation (6.14) is a special case of (6.1). Here $m = 2$, 
so to apply Skolem's method we must have $t > 1$; that is, the equation $f(x, 1) = 0$ must have at least 
one complex root. In such a case we shall say that the form $f(x, y)$ has complex roots. We shall prove 
Thue's theorem by Skolem's method under this assumption. In other words, we shall prove the following assertion.

\subsection*{Theorem 2}
If the form $f(x, y)$ has integral coefficients, is irreducible, has degree $\geq 3$, and has at least one complex root, 
then the equation
\begin{equation}
f(x, y) = c
\end{equation}
has only a finite number of solutions in integers.

\subsection*{Proof.}
We assume that the coefficient $a_0$ of $x^n$ equals $1$ [otherwise we can multiply (6.14) by $a_0^{-1}$ and replace 
$a_0 x^n$ by $x^n$]. Set $k = R(\theta), K = R(\theta_1, \ldots, \theta_n)$, where $\theta = \theta_1, \theta_2, \ldots, 
\theta_n$ are determined by the decomposition
\begin{equation}
f(x, 1) = (x + \theta_1) \cdots (x + \theta_n).
\end{equation}
For each $j = 1, \ldots, n$ we denote by $\sigma_j$ the isomorphism of $K$ to $K$ which takes $\theta$ to $\theta_j$. 
Since $f(x, y) = N(x + y \theta)$ ($N$ denoting the norm of $K / R$), then (6.14) can be written in the form (6.3), where 
by $M$ we mean the module $(1, \theta)$. Hence in this case $\mu_1 = 1, \mu_2 = \theta$ ($m = 2$).

Assume that the equation (6.3) has an infinite number of solutions $\alpha = x + y \theta$ in the module $M = \{1, \theta\}$. 
Then for some $\gamma = \gamma_j \epsilon_i$, an infinite number of these solutions will be of the form (6.5), where the 
independent units $\epsilon_1, \ldots, \epsilon_r$ satisfy Lemma 1. The exponents $u_1, \ldots, u_r$ in (6.5), corresponding 
to each solution $\alpha$, will satisfy the system (6.10). We choose among the solutions $\alpha$ a sequence 
$\alpha_1, \alpha_2, \ldots$ so that the corresponding points $(u_{1s}^*, \ldots, u_{rs}^*)$ $(s = 1, 2, \ldots)$ 
converge to some point $(u_1^*, \ldots, u_r^*)$. In Section 6.2 we saw that the local analytic manifold $V$ defined by 
(6.11) contains an analytic curve $\omega_1(t), \ldots, \omega_r(t)$, and for any such curve on $V$ the series (6.12) 
satisfy some system of the form (6.13).

The rest of the proof of Theorem 2 is based on the following important auxiliary result.




\subsection*{Lemma 2}
Let there be given a system of equations
\begin{equation}
\sum_{j=1}^n a_{ij} \exp P_j = 0 \quad (i = 1, \ldots, n_1),
\end{equation}
\begin{equation}
\sum_{j=1}^n b_{ij} P_j = 0 \quad (i = 1, \ldots, n_2),
\end{equation}
in which each group of equations is linearly independent. If $n_1 = n - 2, n_2 \geq 2$ 
and if the system has a solution in formal power series $P_1(t), \ldots, P_n(t)$ without 
constant term, then $P_i(t) = P_j(t)$ for at least two distinct indices $k$ and $j$. 
[The coefficients $a_{ij}$ and $b_{ij}$, as well as the coefficients of the series 
$P_j(t)$, lie in some fixed field of characteristic zero.]

We give the proof of this lemma below, but now we show how the lemma implies Theorem 2.

By Lemma 2, for any curve $\omega_1(t), \ldots, \omega_r(t)$ on $V$, $P_k(t) = P_j(t)$ 
for at least two distinct indices $j$ and $k$; that is,
\begin{equation}
L_k(\omega_1(t), \ldots, \omega_r(t)) = L_j(\omega_1(t), \ldots, \omega_r(t)).
\end{equation}
In $r$-dimensional space consider the points $(v_1, \ldots, v_r)$ of the manifold $W$ 
which are determined by
\begin{equation}
\prod_{1 \leq k < j \leq n} (L_k(v_1, \ldots, v_r) - L_j(v_1, \ldots, v_r)) = 0.
\end{equation}
It follows from (6.17) that any curve which lies on the local analytic manifold $V$ 
also lies on $W$. But then by Theorem 3 of Section 7, $V \subseteq W$; that is, all 
points of the manifold $V$, contained in some sufficiently small neighborhood of the origin, 
also belong to $W$.

But on the other hand, we shall now show that among the points $(v_{1s}, \ldots, v_{rs}) \in V$, 
which are obtained from the points (6.15) by $u_i = u_i^* + v_i$ and which converge to the origin, 
only a finite number lie on the manifold $W$. This contradiction will prove Theorem 2.

Let $\alpha = x + y \theta$ and $\alpha' = x' + y' \theta$ be two points of the sequence 
$\{\alpha_s\}$, for which the corresponding points of $V$ lie in the manifold determined by 
$L_k = L_j$. If $\alpha = \gamma \epsilon_1^{e_1} \cdots \epsilon_r^{e_r}$ and $u_i = u_i^* + v_i$, then
\begin{equation}
u_i = u_i^* + v_i,
\end{equation}
\begin{equation}
\sigma_j(\alpha) = \sigma_j(\gamma \epsilon_1^{e_1} \cdots \epsilon_r^{e_r})^{u_i} = \sigma_j(\epsilon_k)^{v_r},
\end{equation}
(with $c_j$ independent of $\alpha$) and analogously
\begin{equation}
\sigma_k(\alpha) = c_k \exp L_k(v_1, \ldots, v_r).
\end{equation}




\begin{equation}
\sigma_j(\alpha) = c_j \exp L_j(v_1, \ldots, v_r).
\end{equation}
In precisely the same fashion we find that
\begin{equation}
\sigma_j(\alpha') = \sigma_k(\alpha') = c_k \exp L_k(v_1, \ldots, v_r).
\end{equation}
From the last two equations we obtain
\begin{equation}
\frac{x + y \theta_j}{x' + y' \theta_j} = \frac{x + y \theta_k}{x' + y' \theta_k},
\end{equation}
so that
\begin{equation}
(xy' - x'y)(\theta_k - \theta_j) = 0,
\end{equation}
and since $\theta_j \neq \theta_k$, then
\begin{equation}
xy' - x'y = 0.
\end{equation}
This means that $x + y \theta = d(x' + y' \theta)$ for some rational $d$. Taking norms and 
using the fact that $N(\alpha) = N(\alpha')$, we obtain $d^n = 1$, so that $d = \pm 1$, and
\begin{equation}
\alpha' = \pm \alpha.
\end{equation}
Thus each of the $n(n-1)/2$ manifolds given by $L_k = L_j$, whose union is $W$, contains not 
more than two points of $V$ which correspond to numbers of the sequence $\{\alpha_s\}$. Then 
$W$ contains at most $n(n-1)$ such points. Thus any neighborhood of the origin contains points 
of $V$ not lying on $W$, so $V$ (as a local analytic manifold) is not contained in $W$, which 
contradicts our earlier conclusion that $V \subset W$. As we have noted, this contradiction 
proves Theorem 2.

\subsection*{Proof of Lemma 2}
Since the first group of equations linearly independent (and $n_1 = n - 2), we can, after 
changing the numbering if necessary, express $\exp P_i (i = 1, \ldots, n - 2)$ in terms of 
$\exp P_{n-1}$ and $\exp P_n$:
\begin{equation}
\exp P_i = a_i \exp P_{n-1} + b_i \exp P_n.
\end{equation}
If $a_i = 0, then from the absence of constant terms and the equation $\exp P_i = b_i \exp P_n$, 
we deduce that $b_i = 1$ and $P_i = P_n$. Hence we may assume that all $a_i$ are nonzero. Set
\begin{equation}
P_i - P_n = Q_i \quad (i = 1, \ldots, n - 1)
\end{equation}
and assume that all $Q_i$ are nonzero. By (6.18) we have
\begin{equation}
\exp Q_i = a_i \exp Q_{n-1} + b_i.
\end{equation}



so that by differentiation with respect to $t$ (Problem 10) we obtain
\begin{equation}
Q_i' \exp Q_i = a_i Q_{n-1}' \exp Q_{n-1}.
\end{equation}
From (6.19) and (6.20) we deduce
\begin{equation}
Q_i' = Q_{n-1}' \exp Q_{n-1} \frac{1}{c_i + \exp Q_{n-1}} \quad (i = 1, \ldots, n - 2),
\end{equation}
where $c_i = b_i a_i^{-1}$.

We now use the second group of equations of (6.16). By assumption there are at least two linearly independent equations in this group. Hence we can find a nontrivial relation among the $Q_i$:
\begin{equation}
\sum_{i=1}^{n-1} d_i Q_i = 0.
\end{equation}
Differentiating this identity and replacing $Q_i'$ by (6.21), we obtain
\begin{equation}
Q_{n-1}' \exp Q_{n-1} \left( \sum_{i=1}^{n-2} \frac{d_i}{c_i + \exp Q_{n-1}} + \frac{d_{n-1}}{\exp Q_{n-1}} \right) = 0,
\end{equation}
and since $Q_{n-1}' \neq 0$ and $\exp Q_{n-1} \neq 0$, then
\begin{equation}
\sum_{i=1}^{n-1} \frac{d_i}{c_i + \exp Q_{n-1}} = 0.
\end{equation}
We claim that (6.22) can hold only if the rational function
\begin{equation}
\sum_{i=1}^{n-1} \frac{d_i}{c_i + z}
\end{equation}
is identically zero. Assume that it is not, that is, assume that (6.23) equals $\varphi(z)/\psi(z)$, where $\varphi(z) \neq 0$. Then since $\varphi(\exp Q_{n-1}) = 0$, the nonconstant formal power series $\exp Q_{n-1}$ is the root of a polynomial, which contradicts the assertion of Problem 4 of Section 1. It is clear that the function (6.4) can vanish identically only when $c_k = c_j$ for at least two distinct indices $j$ and $k$. Since $c = ba$, we then find from (6.19) that
\begin{equation}
\exp P_k = \frac{a_k}{a_j} \exp P_j,
\end{equation}
from which it easily follows that $P_k = P_j$. Lemma 2 is proved.

\subsection*{Remark}
Skolem's method allows us to prove that (6.14) has only a finite number of integral solutions. But it does not give an algorithm for finding them.


\end{document}
